{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEPWISE REGRESSION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressWarnings(suppressMessages(install.packages(\"caret\", repos='http://cran.us.r-project.org', dependencies = TRUE)))\n",
    "# suppressWarnings(suppressMessages(install.packages(\"tidyverse\", repos='http://cran.us.r-project.org', dependencies = TRUE)))\n",
    "# suppressWarnings(suppressMessages(install.packages(\"glmnet\", repos='http://cran.us.r-project.org', dependencies = TRUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressWarnings(suppressMessages(library(tidyverse)))\n",
    "suppressWarnings(suppressMessages(library(caret)))\n",
    "suppressWarnings(suppressMessages(library(glmnet)))\n",
    "suppressWarnings(suppressMessages(library(magrittr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressWarnings(suppressMessages(library(MASS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressWarnings(suppressMessages(install.packages(\"leaps\", repos='http://cran.us.r-project.org', dependencies = TRUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressWarnings(suppressMessages(library(leaps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read uscrime data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>M</th><th scope=col>So</th><th scope=col>Ed</th><th scope=col>Po1</th><th scope=col>Po2</th><th scope=col>LF</th><th scope=col>M.F</th><th scope=col>Pop</th><th scope=col>NW</th><th scope=col>U1</th><th scope=col>U2</th><th scope=col>Wealth</th><th scope=col>Ineq</th><th scope=col>Prob</th><th scope=col>Time</th><th scope=col>Crime</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>15.1    </td><td>1       </td><td> 9.1    </td><td> 5.8    </td><td> 5.6    </td><td>0.510   </td><td> 95.0   </td><td> 33     </td><td>30.1    </td><td>0.108   </td><td>4.1     </td><td>3940    </td><td>26.1    </td><td>0.084602</td><td>26.2011 </td><td> 791    </td></tr>\n",
       "\t<tr><td>14.3    </td><td>0       </td><td>11.3    </td><td>10.3    </td><td> 9.5    </td><td>0.583   </td><td>101.2   </td><td> 13     </td><td>10.2    </td><td>0.096   </td><td>3.6     </td><td>5570    </td><td>19.4    </td><td>0.029599</td><td>25.2999 </td><td>1635    </td></tr>\n",
       "\t<tr><td>14.2    </td><td>1       </td><td> 8.9    </td><td> 4.5    </td><td> 4.4    </td><td>0.533   </td><td> 96.9   </td><td> 18     </td><td>21.9    </td><td>0.094   </td><td>3.3     </td><td>3180    </td><td>25.0    </td><td>0.083401</td><td>24.3006 </td><td> 578    </td></tr>\n",
       "\t<tr><td>13.6    </td><td>0       </td><td>12.1    </td><td>14.9    </td><td>14.1    </td><td>0.577   </td><td> 99.4   </td><td>157     </td><td> 8.0    </td><td>0.102   </td><td>3.9     </td><td>6730    </td><td>16.7    </td><td>0.015801</td><td>29.9012 </td><td>1969    </td></tr>\n",
       "\t<tr><td>14.1    </td><td>0       </td><td>12.1    </td><td>10.9    </td><td>10.1    </td><td>0.591   </td><td> 98.5   </td><td> 18     </td><td> 3.0    </td><td>0.091   </td><td>2.0     </td><td>5780    </td><td>17.4    </td><td>0.041399</td><td>21.2998 </td><td>1234    </td></tr>\n",
       "\t<tr><td>12.1    </td><td>0       </td><td>11.0    </td><td>11.8    </td><td>11.5    </td><td>0.547   </td><td> 96.4   </td><td> 25     </td><td> 4.4    </td><td>0.084   </td><td>2.9     </td><td>6890    </td><td>12.6    </td><td>0.034201</td><td>20.9995 </td><td> 682    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       " M & So & Ed & Po1 & Po2 & LF & M.F & Pop & NW & U1 & U2 & Wealth & Ineq & Prob & Time & Crime\\\\\n",
       "\\hline\n",
       "\t 15.1     & 1        &  9.1     &  5.8     &  5.6     & 0.510    &  95.0    &  33      & 30.1     & 0.108    & 4.1      & 3940     & 26.1     & 0.084602 & 26.2011  &  791    \\\\\n",
       "\t 14.3     & 0        & 11.3     & 10.3     &  9.5     & 0.583    & 101.2    &  13      & 10.2     & 0.096    & 3.6      & 5570     & 19.4     & 0.029599 & 25.2999  & 1635    \\\\\n",
       "\t 14.2     & 1        &  8.9     &  4.5     &  4.4     & 0.533    &  96.9    &  18      & 21.9     & 0.094    & 3.3      & 3180     & 25.0     & 0.083401 & 24.3006  &  578    \\\\\n",
       "\t 13.6     & 0        & 12.1     & 14.9     & 14.1     & 0.577    &  99.4    & 157      &  8.0     & 0.102    & 3.9      & 6730     & 16.7     & 0.015801 & 29.9012  & 1969    \\\\\n",
       "\t 14.1     & 0        & 12.1     & 10.9     & 10.1     & 0.591    &  98.5    &  18      &  3.0     & 0.091    & 2.0      & 5780     & 17.4     & 0.041399 & 21.2998  & 1234    \\\\\n",
       "\t 12.1     & 0        & 11.0     & 11.8     & 11.5     & 0.547    &  96.4    &  25      &  4.4     & 0.084    & 2.9      & 6890     & 12.6     & 0.034201 & 20.9995  &  682    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M | So | Ed | Po1 | Po2 | LF | M.F | Pop | NW | U1 | U2 | Wealth | Ineq | Prob | Time | Crime |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 15.1     | 1        |  9.1     |  5.8     |  5.6     | 0.510    |  95.0    |  33      | 30.1     | 0.108    | 4.1      | 3940     | 26.1     | 0.084602 | 26.2011  |  791     |\n",
       "| 14.3     | 0        | 11.3     | 10.3     |  9.5     | 0.583    | 101.2    |  13      | 10.2     | 0.096    | 3.6      | 5570     | 19.4     | 0.029599 | 25.2999  | 1635     |\n",
       "| 14.2     | 1        |  8.9     |  4.5     |  4.4     | 0.533    |  96.9    |  18      | 21.9     | 0.094    | 3.3      | 3180     | 25.0     | 0.083401 | 24.3006  |  578     |\n",
       "| 13.6     | 0        | 12.1     | 14.9     | 14.1     | 0.577    |  99.4    | 157      |  8.0     | 0.102    | 3.9      | 6730     | 16.7     | 0.015801 | 29.9012  | 1969     |\n",
       "| 14.1     | 0        | 12.1     | 10.9     | 10.1     | 0.591    |  98.5    |  18      |  3.0     | 0.091    | 2.0      | 5780     | 17.4     | 0.041399 | 21.2998  | 1234     |\n",
       "| 12.1     | 0        | 11.0     | 11.8     | 11.5     | 0.547    |  96.4    |  25      |  4.4     | 0.084    | 2.9      | 6890     | 12.6     | 0.034201 | 20.9995  |  682     |\n",
       "\n"
      ],
      "text/plain": [
       "  M    So Ed   Po1  Po2  LF    M.F   Pop NW   U1    U2  Wealth Ineq Prob    \n",
       "1 15.1 1   9.1  5.8  5.6 0.510  95.0  33 30.1 0.108 4.1 3940   26.1 0.084602\n",
       "2 14.3 0  11.3 10.3  9.5 0.583 101.2  13 10.2 0.096 3.6 5570   19.4 0.029599\n",
       "3 14.2 1   8.9  4.5  4.4 0.533  96.9  18 21.9 0.094 3.3 3180   25.0 0.083401\n",
       "4 13.6 0  12.1 14.9 14.1 0.577  99.4 157  8.0 0.102 3.9 6730   16.7 0.015801\n",
       "5 14.1 0  12.1 10.9 10.1 0.591  98.5  18  3.0 0.091 2.0 5780   17.4 0.041399\n",
       "6 12.1 0  11.0 11.8 11.5 0.547  96.4  25  4.4 0.084 2.9 6890   12.6 0.034201\n",
       "  Time    Crime\n",
       "1 26.2011  791 \n",
       "2 25.2999 1635 \n",
       "3 24.3006  578 \n",
       "4 29.9012 1969 \n",
       "5 21.2998 1234 \n",
       "6 20.9995  682 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uscrimes <- read.table(\"uscrime.txt\", header=TRUE, sep=\"\\t\")\n",
    "head(uscrimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using stepAIC() function that chooses the best model by AIC. It has an option named direction, which can take the following values: i) “both” (for stepwise regression, both forward and backward\n",
    "selection); “backward” (for backward selection) and “forward” (for forward selection). It return the best final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm.default(formula = Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + \n",
       "    Prob, data = uscrimes)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-444.70 -111.07    3.03  122.15  483.30 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -6426.10    1194.61  -5.379 4.04e-06 ***\n",
       "M              93.32      33.50   2.786  0.00828 ** \n",
       "Ed            180.12      52.75   3.414  0.00153 ** \n",
       "Po1           102.65      15.52   6.613 8.26e-08 ***\n",
       "M.F            22.34      13.60   1.642  0.10874    \n",
       "U1          -6086.63    3339.27  -1.823  0.07622 .  \n",
       "U2            187.35      72.48   2.585  0.01371 *  \n",
       "Ineq           61.33      13.96   4.394 8.63e-05 ***\n",
       "Prob        -3796.03    1490.65  -2.547  0.01505 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 195.5 on 38 degrees of freedom\n",
       "Multiple R-squared:  0.7888,\tAdjusted R-squared:  0.7444 \n",
       "F-statistic: 17.74 on 8 and 38 DF,  p-value: 1.159e-10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the full model\n",
    "model_0 <- lm(Crime ~., data = uscrimes)\n",
    "# Stepwise regression model\n",
    "stepwise.model <- stepAIC(model_0, direction = \"both\", trace = FALSE)\n",
    "summary(stepwise.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next,I am using the function regsubsets(), which has the tuning parameter nvmax specifying the maximal number of predictors to incorporate in the model**\n",
    "\n",
    "**regsubsets() has the option method, which can take the values “backward”, “forward” and “seqrep” (seqrep = sequential replacement, combination of forward and backward selections).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "models <- regsubsets(Crime~., data = uscrimes, nvmax = 15,method = \"seqrep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>nvmax</th><th scope=col>RMSE</th><th scope=col>Rsquared</th><th scope=col>MAE</th><th scope=col>RMSESD</th><th scope=col>RsquaredSD</th><th scope=col>MAESD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1       </td><td>294.0919 </td><td>0.6113474</td><td>234.5452 </td><td> 89.54470</td><td>0.3090369</td><td>68.56939 </td></tr>\n",
       "\t<tr><td> 2       </td><td>276.3968 </td><td>0.6873485</td><td>215.2010 </td><td>106.43356</td><td>0.3252874</td><td>78.51814 </td></tr>\n",
       "\t<tr><td> 3       </td><td>261.9830 </td><td>0.6642742</td><td>210.9873 </td><td>110.59097</td><td>0.2290087</td><td>93.52673 </td></tr>\n",
       "\t<tr><td> 4       </td><td>271.9430 </td><td>0.6397970</td><td>222.7241 </td><td>104.89255</td><td>0.2758066</td><td>74.98079 </td></tr>\n",
       "\t<tr><td> 5       </td><td>260.3804 </td><td>0.5930318</td><td>209.8459 </td><td> 91.58884</td><td>0.2992502</td><td>69.13846 </td></tr>\n",
       "\t<tr><td> 6       </td><td>232.9240 </td><td>0.7379318</td><td>188.7075 </td><td> 97.13428</td><td>0.2237860</td><td>70.61600 </td></tr>\n",
       "\t<tr><td> 7       </td><td>237.8892 </td><td>0.6602768</td><td>197.7499 </td><td> 90.42968</td><td>0.2886561</td><td>67.34118 </td></tr>\n",
       "\t<tr><td> 8       </td><td>276.3432 </td><td>0.5354045</td><td>226.0451 </td><td>107.67461</td><td>0.3665290</td><td>78.50840 </td></tr>\n",
       "\t<tr><td> 9       </td><td>246.8846 </td><td>0.6975119</td><td>201.3635 </td><td> 92.02137</td><td>0.2958348</td><td>67.53324 </td></tr>\n",
       "\t<tr><td>10       </td><td>257.4864 </td><td>0.6694090</td><td>212.3040 </td><td> 96.25332</td><td>0.3222326</td><td>70.34452 </td></tr>\n",
       "\t<tr><td>11       </td><td>277.6348 </td><td>0.6351337</td><td>227.1348 </td><td>119.88247</td><td>0.3316069</td><td>94.32544 </td></tr>\n",
       "\t<tr><td>12       </td><td>258.5886 </td><td>0.6554027</td><td>212.9542 </td><td>107.97925</td><td>0.3213458</td><td>83.31237 </td></tr>\n",
       "\t<tr><td>13       </td><td>259.7346 </td><td>0.6481182</td><td>210.8169 </td><td>107.96438</td><td>0.2963754</td><td>81.43115 </td></tr>\n",
       "\t<tr><td>14       </td><td>249.7601 </td><td>0.6779889</td><td>203.4150 </td><td>108.98821</td><td>0.2951360</td><td>81.59758 </td></tr>\n",
       "\t<tr><td>15       </td><td>255.2604 </td><td>0.6620912</td><td>207.1917 </td><td>107.49455</td><td>0.2940859</td><td>81.54819 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " nvmax & RMSE & Rsquared & MAE & RMSESD & RsquaredSD & MAESD\\\\\n",
       "\\hline\n",
       "\t  1        & 294.0919  & 0.6113474 & 234.5452  &  89.54470 & 0.3090369 & 68.56939 \\\\\n",
       "\t  2        & 276.3968  & 0.6873485 & 215.2010  & 106.43356 & 0.3252874 & 78.51814 \\\\\n",
       "\t  3        & 261.9830  & 0.6642742 & 210.9873  & 110.59097 & 0.2290087 & 93.52673 \\\\\n",
       "\t  4        & 271.9430  & 0.6397970 & 222.7241  & 104.89255 & 0.2758066 & 74.98079 \\\\\n",
       "\t  5        & 260.3804  & 0.5930318 & 209.8459  &  91.58884 & 0.2992502 & 69.13846 \\\\\n",
       "\t  6        & 232.9240  & 0.7379318 & 188.7075  &  97.13428 & 0.2237860 & 70.61600 \\\\\n",
       "\t  7        & 237.8892  & 0.6602768 & 197.7499  &  90.42968 & 0.2886561 & 67.34118 \\\\\n",
       "\t  8        & 276.3432  & 0.5354045 & 226.0451  & 107.67461 & 0.3665290 & 78.50840 \\\\\n",
       "\t  9        & 246.8846  & 0.6975119 & 201.3635  &  92.02137 & 0.2958348 & 67.53324 \\\\\n",
       "\t 10        & 257.4864  & 0.6694090 & 212.3040  &  96.25332 & 0.3222326 & 70.34452 \\\\\n",
       "\t 11        & 277.6348  & 0.6351337 & 227.1348  & 119.88247 & 0.3316069 & 94.32544 \\\\\n",
       "\t 12        & 258.5886  & 0.6554027 & 212.9542  & 107.97925 & 0.3213458 & 83.31237 \\\\\n",
       "\t 13        & 259.7346  & 0.6481182 & 210.8169  & 107.96438 & 0.2963754 & 81.43115 \\\\\n",
       "\t 14        & 249.7601  & 0.6779889 & 203.4150  & 108.98821 & 0.2951360 & 81.59758 \\\\\n",
       "\t 15        & 255.2604  & 0.6620912 & 207.1917  & 107.49455 & 0.2940859 & 81.54819 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| nvmax | RMSE | Rsquared | MAE | RMSESD | RsquaredSD | MAESD |\n",
       "|---|---|---|---|---|---|---|\n",
       "|  1        | 294.0919  | 0.6113474 | 234.5452  |  89.54470 | 0.3090369 | 68.56939  |\n",
       "|  2        | 276.3968  | 0.6873485 | 215.2010  | 106.43356 | 0.3252874 | 78.51814  |\n",
       "|  3        | 261.9830  | 0.6642742 | 210.9873  | 110.59097 | 0.2290087 | 93.52673  |\n",
       "|  4        | 271.9430  | 0.6397970 | 222.7241  | 104.89255 | 0.2758066 | 74.98079  |\n",
       "|  5        | 260.3804  | 0.5930318 | 209.8459  |  91.58884 | 0.2992502 | 69.13846  |\n",
       "|  6        | 232.9240  | 0.7379318 | 188.7075  |  97.13428 | 0.2237860 | 70.61600  |\n",
       "|  7        | 237.8892  | 0.6602768 | 197.7499  |  90.42968 | 0.2886561 | 67.34118  |\n",
       "|  8        | 276.3432  | 0.5354045 | 226.0451  | 107.67461 | 0.3665290 | 78.50840  |\n",
       "|  9        | 246.8846  | 0.6975119 | 201.3635  |  92.02137 | 0.2958348 | 67.53324  |\n",
       "| 10        | 257.4864  | 0.6694090 | 212.3040  |  96.25332 | 0.3222326 | 70.34452  |\n",
       "| 11        | 277.6348  | 0.6351337 | 227.1348  | 119.88247 | 0.3316069 | 94.32544  |\n",
       "| 12        | 258.5886  | 0.6554027 | 212.9542  | 107.97925 | 0.3213458 | 83.31237  |\n",
       "| 13        | 259.7346  | 0.6481182 | 210.8169  | 107.96438 | 0.2963754 | 81.43115  |\n",
       "| 14        | 249.7601  | 0.6779889 | 203.4150  | 108.98821 | 0.2951360 | 81.59758  |\n",
       "| 15        | 255.2604  | 0.6620912 | 207.1917  | 107.49455 | 0.2940859 | 81.54819  |\n",
       "\n"
      ],
      "text/plain": [
       "   nvmax RMSE     Rsquared  MAE      RMSESD    RsquaredSD MAESD   \n",
       "1   1    294.0919 0.6113474 234.5452  89.54470 0.3090369  68.56939\n",
       "2   2    276.3968 0.6873485 215.2010 106.43356 0.3252874  78.51814\n",
       "3   3    261.9830 0.6642742 210.9873 110.59097 0.2290087  93.52673\n",
       "4   4    271.9430 0.6397970 222.7241 104.89255 0.2758066  74.98079\n",
       "5   5    260.3804 0.5930318 209.8459  91.58884 0.2992502  69.13846\n",
       "6   6    232.9240 0.7379318 188.7075  97.13428 0.2237860  70.61600\n",
       "7   7    237.8892 0.6602768 197.7499  90.42968 0.2886561  67.34118\n",
       "8   8    276.3432 0.5354045 226.0451 107.67461 0.3665290  78.50840\n",
       "9   9    246.8846 0.6975119 201.3635  92.02137 0.2958348  67.53324\n",
       "10 10    257.4864 0.6694090 212.3040  96.25332 0.3222326  70.34452\n",
       "11 11    277.6348 0.6351337 227.1348 119.88247 0.3316069  94.32544\n",
       "12 12    258.5886 0.6554027 212.9542 107.97925 0.3213458  83.31237\n",
       "13 13    259.7346 0.6481182 210.8169 107.96438 0.2963754  81.43115\n",
       "14 14    249.7601 0.6779889 203.4150 108.98821 0.2951360  81.59758\n",
       "15 15    255.2604 0.6620912 207.1917 107.49455 0.2940859  81.54819"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Set seed for reproducibility\n",
    "set.seed(123)\n",
    "# Set up repeated k-fold cross-validation\n",
    "train.control <- trainControl(method = \"cv\", number = 10)\n",
    "# Train the model\n",
    "step.model <- train(Crime ~., data = uscrimes,\n",
    "                    method = \"leapSeq\",\n",
    "                    tuneGrid = data.frame(nvmax = 1:15),\n",
    "                    trControl = train.control\n",
    "                   )\n",
    "step.model$results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nvmax = 6 has the lowest RMSE. Next, I am going to display the best tuning values (nvmax), automatically selected by the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>nvmax</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>6</th><td>6</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & nvmax\\\\\n",
       "\\hline\n",
       "\t6 & 6\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | nvmax |\n",
       "|---|---|\n",
       "| 6 | 6 |\n",
       "\n"
      ],
      "text/plain": [
       "  nvmax\n",
       "6 6    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step.model$bestTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This indicates that the best model is the one with nvmax = 6 variables. The function summary() reports the best set of variables for each model size, up to the best 6-variables model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subset selection object\n",
       "15 Variables  (and intercept)\n",
       "       Forced in Forced out\n",
       "M          FALSE      FALSE\n",
       "So         FALSE      FALSE\n",
       "Ed         FALSE      FALSE\n",
       "Po1        FALSE      FALSE\n",
       "Po2        FALSE      FALSE\n",
       "LF         FALSE      FALSE\n",
       "M.F        FALSE      FALSE\n",
       "Pop        FALSE      FALSE\n",
       "NW         FALSE      FALSE\n",
       "U1         FALSE      FALSE\n",
       "U2         FALSE      FALSE\n",
       "Wealth     FALSE      FALSE\n",
       "Ineq       FALSE      FALSE\n",
       "Prob       FALSE      FALSE\n",
       "Time       FALSE      FALSE\n",
       "1 subsets of each size up to 6\n",
       "Selection Algorithm: 'sequential replacement'\n",
       "         M   So  Ed  Po1 Po2 LF  M.F Pop NW  U1  U2  Wealth Ineq Prob Time\n",
       "1  ( 1 ) \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"    \" \"  \" \"  \" \" \n",
       "2  ( 1 ) \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"    \"*\"  \" \"  \" \" \n",
       "3  ( 1 ) \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"    \"*\"  \" \"  \" \" \n",
       "4  ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"    \" \"  \" \"  \" \" \n",
       "5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"    \"*\"  \"*\"  \" \" \n",
       "6  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \"*\" \" \"    \"*\"  \"*\"  \" \" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(step.model$finalModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An asterisk specifies that a given variable is included in the corresponding model.**\n",
    "\n",
    "**For example, it can be seen that the best 6-variable model contains M+Ed+Po1+U2+Ineq+Prob (Crime ~ M+Ed+Po1+U2+Ineq+Prob).**\n",
    "\n",
    "**The regression coefficients of the final model (id = 6) is:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>-5040.50497740906</dd>\n",
       "\t<dt>M</dt>\n",
       "\t\t<dd>105.019567900143</dd>\n",
       "\t<dt>Ed</dt>\n",
       "\t\t<dd>196.471200528952</dd>\n",
       "\t<dt>Po1</dt>\n",
       "\t\t<dd>115.024190750002</dd>\n",
       "\t<dt>U2</dt>\n",
       "\t\t<dd>89.3660430869501</dd>\n",
       "\t<dt>Ineq</dt>\n",
       "\t\t<dd>67.6532158858335</dd>\n",
       "\t<dt>Prob</dt>\n",
       "\t\t<dd>-3801.83627942786</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] -5040.50497740906\n",
       "\\item[M] 105.019567900143\n",
       "\\item[Ed] 196.471200528952\n",
       "\\item[Po1] 115.024190750002\n",
       "\\item[U2] 89.3660430869501\n",
       "\\item[Ineq] 67.6532158858335\n",
       "\\item[Prob] -3801.83627942786\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   -5040.50497740906M\n",
       ":   105.019567900143Ed\n",
       ":   196.471200528952Po1\n",
       ":   115.024190750002U2\n",
       ":   89.3660430869501Ineq\n",
       ":   67.6532158858335Prob\n",
       ":   -3801.83627942786\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)           M          Ed         Po1          U2        Ineq \n",
       "-5040.50498   105.01957   196.47120   115.02419    89.36604    67.65322 \n",
       "       Prob \n",
       "-3801.83628 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef(step.model$finalModel, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The regression coefficients can also be found out by computing the linear model using only the selected predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm.default(formula = Crime ~ M + Ed + Po1 + U2 + Ineq + Prob, \n",
       "    data = uscrimes)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-470.68  -78.41  -19.68  133.12  556.23 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -5040.50     899.84  -5.602 1.72e-06 ***\n",
       "M             105.02      33.30   3.154  0.00305 ** \n",
       "Ed            196.47      44.75   4.390 8.07e-05 ***\n",
       "Po1           115.02      13.75   8.363 2.56e-10 ***\n",
       "U2             89.37      40.91   2.185  0.03483 *  \n",
       "Ineq           67.65      13.94   4.855 1.88e-05 ***\n",
       "Prob        -3801.84    1528.10  -2.488  0.01711 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 200.7 on 40 degrees of freedom\n",
       "Multiple R-squared:  0.7659,\tAdjusted R-squared:  0.7307 \n",
       "F-statistic: 21.81 on 6 and 40 DF,  p-value: 3.418e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1 <- lm(Crime ~ M+Ed+Po1+U2+Ineq+Prob, data = uscrimes)\n",
    "summary(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using this approach returns M, Ed, Po1, U2, Ineq, Prob as significant predictors. Adjusted R squared is 0.7307."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LASSO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am going to randomly split the data into training set (80% for building a predictive model) and test set (20% for evaluating the model).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "training.samples <- uscrimes$Crime %>% createDataPartition(p = 0.8, list = FALSE)\n",
    "train.data <- uscrimes[training.samples, ]\n",
    "test.data <- uscrimes[-training.samples, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The R function model.matrix() helps to create the matrix of predictors and also automatically converts categorical predictors to appropriate dummy variables, which is required for the glmnet()\n",
    "function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- scale(model.matrix(Crime~., train.data)[,-1])\n",
    "y <- train.data$Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am going to use the R function glmnet() for computing penalized logistic regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = 10^seq(10, -2, length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1e+10</li>\n",
       "\t<li>7564633275.54629</li>\n",
       "\t<li>5722367659.35022</li>\n",
       "\t<li>4328761281.08306</li>\n",
       "\t<li>3274549162.87773</li>\n",
       "\t<li>2477076355.99171</li>\n",
       "\t<li>1873817422.86039</li>\n",
       "\t<li>1417474162.92681</li>\n",
       "\t<li>1072267222.01033</li>\n",
       "\t<li>811130830.789689</li>\n",
       "\t<li>613590727.341316</li>\n",
       "\t<li>464158883.361277</li>\n",
       "\t<li>351119173.421513</li>\n",
       "\t<li>265608778.294668</li>\n",
       "\t<li>200923300.256505</li>\n",
       "\t<li>151991108.295293</li>\n",
       "\t<li>114975699.539774</li>\n",
       "\t<li>86974900.2617783</li>\n",
       "\t<li>65793322.4657568</li>\n",
       "\t<li>49770235.6433211</li>\n",
       "\t<li>37649358.0679247</li>\n",
       "\t<li>28480358.684358</li>\n",
       "\t<li>21544346.9003188</li>\n",
       "\t<li>16297508.3462064</li>\n",
       "\t<li>12328467.3944207</li>\n",
       "\t<li>9326033.4688322</li>\n",
       "\t<li>7054802.31071865</li>\n",
       "\t<li>5336699.2312063</li>\n",
       "\t<li>4037017.25859655</li>\n",
       "\t<li>3053855.50883341</li>\n",
       "\t<li>2310129.70008316</li>\n",
       "\t<li>1747528.40000768</li>\n",
       "\t<li>1321941.14846603</li>\n",
       "\t<li>1e+06</li>\n",
       "\t<li>756463.327554629</li>\n",
       "\t<li>572236.765935022</li>\n",
       "\t<li>432876.128108306</li>\n",
       "\t<li>327454.916287772</li>\n",
       "\t<li>247707.635599171</li>\n",
       "\t<li>187381.742286038</li>\n",
       "\t<li>141747.41629268</li>\n",
       "\t<li>107226.722201032</li>\n",
       "\t<li>81113.0830789687</li>\n",
       "\t<li>61359.0727341318</li>\n",
       "\t<li>46415.8883361277</li>\n",
       "\t<li>35111.9173421513</li>\n",
       "\t<li>26560.8778294668</li>\n",
       "\t<li>20092.3300256505</li>\n",
       "\t<li>15199.1108295293</li>\n",
       "\t<li>11497.5699539774</li>\n",
       "\t<li>8697.49002617783</li>\n",
       "\t<li>6579.33224657568</li>\n",
       "\t<li>4977.02356433211</li>\n",
       "\t<li>3764.93580679246</li>\n",
       "\t<li>2848.0358684358</li>\n",
       "\t<li>2154.43469003188</li>\n",
       "\t<li>1629.75083462064</li>\n",
       "\t<li>1232.84673944207</li>\n",
       "\t<li>932.60334688322</li>\n",
       "\t<li>705.480231071865</li>\n",
       "\t<li>533.66992312063</li>\n",
       "\t<li>403.701725859655</li>\n",
       "\t<li>305.385550883341</li>\n",
       "\t<li>231.012970008316</li>\n",
       "\t<li>174.752840000768</li>\n",
       "\t<li>132.194114846603</li>\n",
       "\t<li>100</li>\n",
       "\t<li>75.6463327554629</li>\n",
       "\t<li>57.2236765935022</li>\n",
       "\t<li>43.2876128108306</li>\n",
       "\t<li>32.7454916287773</li>\n",
       "\t<li>24.7707635599171</li>\n",
       "\t<li>18.7381742286039</li>\n",
       "\t<li>14.174741629268</li>\n",
       "\t<li>10.7226722201032</li>\n",
       "\t<li>8.11130830789686</li>\n",
       "\t<li>6.13590727341316</li>\n",
       "\t<li>4.64158883361277</li>\n",
       "\t<li>3.51119173421513</li>\n",
       "\t<li>2.65608778294668</li>\n",
       "\t<li>2.00923300256505</li>\n",
       "\t<li>1.51991108295293</li>\n",
       "\t<li>1.14975699539774</li>\n",
       "\t<li>0.869749002617783</li>\n",
       "\t<li>0.657933224657568</li>\n",
       "\t<li>0.497702356433211</li>\n",
       "\t<li>0.376493580679247</li>\n",
       "\t<li>0.28480358684358</li>\n",
       "\t<li>0.215443469003188</li>\n",
       "\t<li>0.162975083462064</li>\n",
       "\t<li>0.123284673944206</li>\n",
       "\t<li>0.0932603346883218</li>\n",
       "\t<li>0.0705480231071863</li>\n",
       "\t<li>0.053366992312063</li>\n",
       "\t<li>0.0403701725859655</li>\n",
       "\t<li>0.0305385550883341</li>\n",
       "\t<li>0.0231012970008316</li>\n",
       "\t<li>0.0174752840000768</li>\n",
       "\t<li>0.0132194114846603</li>\n",
       "\t<li>0.01</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1e+10\n",
       "\\item 7564633275.54629\n",
       "\\item 5722367659.35022\n",
       "\\item 4328761281.08306\n",
       "\\item 3274549162.87773\n",
       "\\item 2477076355.99171\n",
       "\\item 1873817422.86039\n",
       "\\item 1417474162.92681\n",
       "\\item 1072267222.01033\n",
       "\\item 811130830.789689\n",
       "\\item 613590727.341316\n",
       "\\item 464158883.361277\n",
       "\\item 351119173.421513\n",
       "\\item 265608778.294668\n",
       "\\item 200923300.256505\n",
       "\\item 151991108.295293\n",
       "\\item 114975699.539774\n",
       "\\item 86974900.2617783\n",
       "\\item 65793322.4657568\n",
       "\\item 49770235.6433211\n",
       "\\item 37649358.0679247\n",
       "\\item 28480358.684358\n",
       "\\item 21544346.9003188\n",
       "\\item 16297508.3462064\n",
       "\\item 12328467.3944207\n",
       "\\item 9326033.4688322\n",
       "\\item 7054802.31071865\n",
       "\\item 5336699.2312063\n",
       "\\item 4037017.25859655\n",
       "\\item 3053855.50883341\n",
       "\\item 2310129.70008316\n",
       "\\item 1747528.40000768\n",
       "\\item 1321941.14846603\n",
       "\\item 1e+06\n",
       "\\item 756463.327554629\n",
       "\\item 572236.765935022\n",
       "\\item 432876.128108306\n",
       "\\item 327454.916287772\n",
       "\\item 247707.635599171\n",
       "\\item 187381.742286038\n",
       "\\item 141747.41629268\n",
       "\\item 107226.722201032\n",
       "\\item 81113.0830789687\n",
       "\\item 61359.0727341318\n",
       "\\item 46415.8883361277\n",
       "\\item 35111.9173421513\n",
       "\\item 26560.8778294668\n",
       "\\item 20092.3300256505\n",
       "\\item 15199.1108295293\n",
       "\\item 11497.5699539774\n",
       "\\item 8697.49002617783\n",
       "\\item 6579.33224657568\n",
       "\\item 4977.02356433211\n",
       "\\item 3764.93580679246\n",
       "\\item 2848.0358684358\n",
       "\\item 2154.43469003188\n",
       "\\item 1629.75083462064\n",
       "\\item 1232.84673944207\n",
       "\\item 932.60334688322\n",
       "\\item 705.480231071865\n",
       "\\item 533.66992312063\n",
       "\\item 403.701725859655\n",
       "\\item 305.385550883341\n",
       "\\item 231.012970008316\n",
       "\\item 174.752840000768\n",
       "\\item 132.194114846603\n",
       "\\item 100\n",
       "\\item 75.6463327554629\n",
       "\\item 57.2236765935022\n",
       "\\item 43.2876128108306\n",
       "\\item 32.7454916287773\n",
       "\\item 24.7707635599171\n",
       "\\item 18.7381742286039\n",
       "\\item 14.174741629268\n",
       "\\item 10.7226722201032\n",
       "\\item 8.11130830789686\n",
       "\\item 6.13590727341316\n",
       "\\item 4.64158883361277\n",
       "\\item 3.51119173421513\n",
       "\\item 2.65608778294668\n",
       "\\item 2.00923300256505\n",
       "\\item 1.51991108295293\n",
       "\\item 1.14975699539774\n",
       "\\item 0.869749002617783\n",
       "\\item 0.657933224657568\n",
       "\\item 0.497702356433211\n",
       "\\item 0.376493580679247\n",
       "\\item 0.28480358684358\n",
       "\\item 0.215443469003188\n",
       "\\item 0.162975083462064\n",
       "\\item 0.123284673944206\n",
       "\\item 0.0932603346883218\n",
       "\\item 0.0705480231071863\n",
       "\\item 0.053366992312063\n",
       "\\item 0.0403701725859655\n",
       "\\item 0.0305385550883341\n",
       "\\item 0.0231012970008316\n",
       "\\item 0.0174752840000768\n",
       "\\item 0.0132194114846603\n",
       "\\item 0.01\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1e+10\n",
       "2. 7564633275.54629\n",
       "3. 5722367659.35022\n",
       "4. 4328761281.08306\n",
       "5. 3274549162.87773\n",
       "6. 2477076355.99171\n",
       "7. 1873817422.86039\n",
       "8. 1417474162.92681\n",
       "9. 1072267222.01033\n",
       "10. 811130830.789689\n",
       "11. 613590727.341316\n",
       "12. 464158883.361277\n",
       "13. 351119173.421513\n",
       "14. 265608778.294668\n",
       "15. 200923300.256505\n",
       "16. 151991108.295293\n",
       "17. 114975699.539774\n",
       "18. 86974900.2617783\n",
       "19. 65793322.4657568\n",
       "20. 49770235.6433211\n",
       "21. 37649358.0679247\n",
       "22. 28480358.684358\n",
       "23. 21544346.9003188\n",
       "24. 16297508.3462064\n",
       "25. 12328467.3944207\n",
       "26. 9326033.4688322\n",
       "27. 7054802.31071865\n",
       "28. 5336699.2312063\n",
       "29. 4037017.25859655\n",
       "30. 3053855.50883341\n",
       "31. 2310129.70008316\n",
       "32. 1747528.40000768\n",
       "33. 1321941.14846603\n",
       "34. 1e+06\n",
       "35. 756463.327554629\n",
       "36. 572236.765935022\n",
       "37. 432876.128108306\n",
       "38. 327454.916287772\n",
       "39. 247707.635599171\n",
       "40. 187381.742286038\n",
       "41. 141747.41629268\n",
       "42. 107226.722201032\n",
       "43. 81113.0830789687\n",
       "44. 61359.0727341318\n",
       "45. 46415.8883361277\n",
       "46. 35111.9173421513\n",
       "47. 26560.8778294668\n",
       "48. 20092.3300256505\n",
       "49. 15199.1108295293\n",
       "50. 11497.5699539774\n",
       "51. 8697.49002617783\n",
       "52. 6579.33224657568\n",
       "53. 4977.02356433211\n",
       "54. 3764.93580679246\n",
       "55. 2848.0358684358\n",
       "56. 2154.43469003188\n",
       "57. 1629.75083462064\n",
       "58. 1232.84673944207\n",
       "59. 932.60334688322\n",
       "60. 705.480231071865\n",
       "61. 533.66992312063\n",
       "62. 403.701725859655\n",
       "63. 305.385550883341\n",
       "64. 231.012970008316\n",
       "65. 174.752840000768\n",
       "66. 132.194114846603\n",
       "67. 100\n",
       "68. 75.6463327554629\n",
       "69. 57.2236765935022\n",
       "70. 43.2876128108306\n",
       "71. 32.7454916287773\n",
       "72. 24.7707635599171\n",
       "73. 18.7381742286039\n",
       "74. 14.174741629268\n",
       "75. 10.7226722201032\n",
       "76. 8.11130830789686\n",
       "77. 6.13590727341316\n",
       "78. 4.64158883361277\n",
       "79. 3.51119173421513\n",
       "80. 2.65608778294668\n",
       "81. 2.00923300256505\n",
       "82. 1.51991108295293\n",
       "83. 1.14975699539774\n",
       "84. 0.869749002617783\n",
       "85. 0.657933224657568\n",
       "86. 0.497702356433211\n",
       "87. 0.376493580679247\n",
       "88. 0.28480358684358\n",
       "89. 0.215443469003188\n",
       "90. 0.162975083462064\n",
       "91. 0.123284673944206\n",
       "92. 0.0932603346883218\n",
       "93. 0.0705480231071863\n",
       "94. 0.053366992312063\n",
       "95. 0.0403701725859655\n",
       "96. 0.0305385550883341\n",
       "97. 0.0231012970008316\n",
       "98. 0.0174752840000768\n",
       "99. 0.0132194114846603\n",
       "100. 0.01\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 1.000000e+10 7.564633e+09 5.722368e+09 4.328761e+09 3.274549e+09\n",
       "  [6] 2.477076e+09 1.873817e+09 1.417474e+09 1.072267e+09 8.111308e+08\n",
       " [11] 6.135907e+08 4.641589e+08 3.511192e+08 2.656088e+08 2.009233e+08\n",
       " [16] 1.519911e+08 1.149757e+08 8.697490e+07 6.579332e+07 4.977024e+07\n",
       " [21] 3.764936e+07 2.848036e+07 2.154435e+07 1.629751e+07 1.232847e+07\n",
       " [26] 9.326033e+06 7.054802e+06 5.336699e+06 4.037017e+06 3.053856e+06\n",
       " [31] 2.310130e+06 1.747528e+06 1.321941e+06 1.000000e+06 7.564633e+05\n",
       " [36] 5.722368e+05 4.328761e+05 3.274549e+05 2.477076e+05 1.873817e+05\n",
       " [41] 1.417474e+05 1.072267e+05 8.111308e+04 6.135907e+04 4.641589e+04\n",
       " [46] 3.511192e+04 2.656088e+04 2.009233e+04 1.519911e+04 1.149757e+04\n",
       " [51] 8.697490e+03 6.579332e+03 4.977024e+03 3.764936e+03 2.848036e+03\n",
       " [56] 2.154435e+03 1.629751e+03 1.232847e+03 9.326033e+02 7.054802e+02\n",
       " [61] 5.336699e+02 4.037017e+02 3.053856e+02 2.310130e+02 1.747528e+02\n",
       " [66] 1.321941e+02 1.000000e+02 7.564633e+01 5.722368e+01 4.328761e+01\n",
       " [71] 3.274549e+01 2.477076e+01 1.873817e+01 1.417474e+01 1.072267e+01\n",
       " [76] 8.111308e+00 6.135907e+00 4.641589e+00 3.511192e+00 2.656088e+00\n",
       " [81] 2.009233e+00 1.519911e+00 1.149757e+00 8.697490e-01 6.579332e-01\n",
       " [86] 4.977024e-01 3.764936e-01 2.848036e-01 2.154435e-01 1.629751e-01\n",
       " [91] 1.232847e-01 9.326033e-02 7.054802e-02 5.336699e-02 4.037017e-02\n",
       " [96] 3.053856e-02 2.310130e-02 1.747528e-02 1.321941e-02 1.000000e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best lambda using cross-validation\n",
    "set.seed(123)\n",
    "cv.lasso <- cv.glmnet(x, y, alpha = 1, family = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final model on the training data\n",
    "model <- glmnet(x, y, alpha = 1, family = \"gaussian\", lambda = lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.model <- cv.glmnet(x, y, alpha = 1, lambda = lambda, nfolds = 10, family = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(lasso.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The plot displays the cross-validation error according to the log of lambda. The plot indicates that the log of the optimal value of lambda is approximately closer to zero, which is the one that minimizes the prediction error. This lambda value will give the most accurate model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The exact value of lambda is:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "18.7381742286039"
      ],
      "text/latex": [
       "18.7381742286039"
      ],
      "text/markdown": [
       "18.7381742286039"
      ],
      "text/plain": [
       "[1] 18.73817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lambda <- lasso.model$lambda.min\n",
    "best_lambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using this value of lambda as the best lambda, we can get the regression coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                     1\n",
       "(Intercept) 902.128205\n",
       "M            59.101431\n",
       "So           12.136175\n",
       "Ed           29.374718\n",
       "Po1         254.274825\n",
       "Po2           .       \n",
       "LF            .       \n",
       "M.F          56.113940\n",
       "Pop           .       \n",
       "NW            .       \n",
       "U1            .       \n",
       "U2            5.486144\n",
       "Wealth        .       \n",
       "Ineq        123.247824\n",
       "Prob        -88.775754\n",
       "Time          .       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef(lasso.model, best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, I am going to fit a model with the predictors (M + So+ Ed + Po1 + M.F + U2 + Ineq + Prob) using best_lambda and then assess the model accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm.default(formula = Crime ~ M + So + Ed + Po1 + M.F + U2 + Ineq + \n",
       "    Prob, data = uscrimes)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-416.39 -121.33   -1.36  114.57  559.34 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -5711.41    1156.48  -4.939 1.61e-05 ***\n",
       "M              88.43      36.05   2.453 0.018861 *  \n",
       "So            101.46     113.17   0.897 0.375614    \n",
       "Ed            175.80      54.51   3.225 0.002588 ** \n",
       "Po1           112.71      14.60   7.721 2.66e-09 ***\n",
       "M.F            13.56      12.80   1.060 0.296030    \n",
       "U2             75.17      43.03   1.747 0.088723 .  \n",
       "Ineq           59.93      15.58   3.848 0.000442 ***\n",
       "Prob        -4457.71    1676.95  -2.658 0.011430 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 201.8 on 38 degrees of freedom\n",
       "Multiple R-squared:  0.7751,\tAdjusted R-squared:  0.7278 \n",
       "F-statistic: 16.37 on 8 and 38 DF,  p-value: 3.645e-10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso.model1 <- lm(Crime ~ M + So+ Ed + Po1 + M.F + U2 + Ineq + Prob, data = uscrimes)\n",
    "summary(lasso.model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using LASSO approach, returns lambda = 18.74. The most significant predictors are M + So+ Ed + Po1 + M.F + U2 + Ineq + Prob. Adjusted R squared is 0.7278. p_value is much less than 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will try the LASSO approach using caret package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "set.seed(123)\n",
    "lasso <- train(\n",
    "    Crime ~., data = train.data, method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 10),\n",
    "    tuneGrid = expand.grid(alpha = 1, lambda = lambda)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                        1\n",
       "(Intercept) -3.777694e+03\n",
       "M            5.833848e+01\n",
       "So           3.266619e+01\n",
       "Ed           6.477957e+01\n",
       "Po1          9.428008e+01\n",
       "Po2          .           \n",
       "LF           .           \n",
       "M.F          1.784047e+01\n",
       "Pop          .           \n",
       "NW           .           \n",
       "U1           .           \n",
       "U2           2.494414e+01\n",
       "Wealth       1.379219e-06\n",
       "Ineq         3.963253e+01\n",
       "Prob        -4.973702e+03\n",
       "Time         .           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model coefficients\n",
    "coef(lasso$finalModel, lasso$bestTune$lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "14.174741629268"
      ],
      "text/latex": [
       "14.174741629268"
      ],
      "text/markdown": [
       "14.174741629268"
      ],
      "text/plain": [
       "[1] 14.17474"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso$bestTune$lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using caret package yields lambda = 14.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions <- lasso %>% predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>471.240245009336</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>874.714937365974</dd>\n",
       "\t<dt>18</dt>\n",
       "\t\t<dd>763.578270346108</dd>\n",
       "\t<dt>20</dt>\n",
       "\t\t<dd>1103.30241266111</dd>\n",
       "\t<dt>26</dt>\n",
       "\t\t<dd>1686.86986621584</dd>\n",
       "\t<dt>30</dt>\n",
       "\t\t<dd>712.27269743772</dd>\n",
       "\t<dt>31</dt>\n",
       "\t\t<dd>707.907193803766</dd>\n",
       "\t<dt>41</dt>\n",
       "\t\t<dd>831.086683600615</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[3] 471.240245009336\n",
       "\\item[6] 874.714937365974\n",
       "\\item[18] 763.578270346108\n",
       "\\item[20] 1103.30241266111\n",
       "\\item[26] 1686.86986621584\n",
       "\\item[30] 712.27269743772\n",
       "\\item[31] 707.907193803766\n",
       "\\item[41] 831.086683600615\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "3\n",
       ":   471.2402450093366\n",
       ":   874.71493736597418\n",
       ":   763.57827034610820\n",
       ":   1103.3024126611126\n",
       ":   1686.8698662158430\n",
       ":   712.2726974377231\n",
       ":   707.90719380376641\n",
       ":   831.086683600615\n",
       "\n"
      ],
      "text/plain": [
       "        3         6        18        20        26        30        31        41 \n",
       " 471.2402  874.7149  763.5783 1103.3024 1686.8699  712.2727  707.9072  831.0867 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>RMSE</th><th scope=col>Rsquare</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>193.406  </td><td>0.8768125</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " RMSE & Rsquare\\\\\n",
       "\\hline\n",
       "\t 193.406   & 0.8768125\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| RMSE | Rsquare |\n",
       "|---|---|\n",
       "| 193.406   | 0.8768125 |\n",
       "\n"
      ],
      "text/plain": [
       "  RMSE    Rsquare  \n",
       "1 193.406 0.8768125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Model prediction performance\n",
    "data.frame(\n",
    "    RMSE = RMSE(predictions, test.data$Crime),\n",
    "    Rsquare = R2(predictions, test.data$Crime)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELASTIC NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This approach combines lasso and ridge regression methods. Lambda = 1 and Alpha can vary between 0 and 1.**\n",
    "\n",
    "**I am going to use caret to get the best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "training.samples <- uscrimes$Crime %>% createDataPartition(p = 0.8, list = FALSE)\n",
    "train.data <- uscrimes[training.samples, ]\n",
    "test.data <- uscrimes[-training.samples, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- scale(model.matrix(Crime~., train.data)[,-1])\n",
    "y <- train.data$Crime\n",
    "lambda = 10^seq(10, -2, length = 100)\n",
    "alpha = seq(0.0, 1.0, by = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_control <- trainControl(method=\"cv\", number=10)\n",
    "grid <- expand.grid(alpha = alpha,lambda = lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    }
   ],
   "source": [
    "elastic_model <- train(x, y, method = \"glmnet\", tuneGrid = grid, trControl = train_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>alpha</th><th scope=col>lambda</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>38</th><td>0       </td><td>305.3856</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & alpha & lambda\\\\\n",
       "\\hline\n",
       "\t38 & 0        & 305.3856\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | alpha | lambda |\n",
       "|---|---|---|\n",
       "| 38 | 0        | 305.3856 |\n",
       "\n"
      ],
      "text/plain": [
       "   alpha lambda  \n",
       "38 0     305.3856"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elastic_model$bestTune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, the best value of alpha = 0 and best value of lambda = 305.3856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coefficients of the elastic net model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                     1\n",
       "(Intercept) 902.128205\n",
       "M            37.421451\n",
       "So           23.477864\n",
       "Ed           24.965912\n",
       "Po1          72.075533\n",
       "Po2          68.814423\n",
       "LF            4.057885\n",
       "M.F          41.323797\n",
       "Pop          26.274182\n",
       "NW           28.178510\n",
       "U1          -13.035689\n",
       "U2           18.329915\n",
       "Wealth       24.667759\n",
       "Ineq         37.019598\n",
       "Prob        -62.980711\n",
       "Time          3.841732"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef(elastic_model$finalModel, elastic_model$bestTune$lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, I am going to fit the model with the predictors (M + So+ Po1 + Po2 + M.F + NW + Ineq + Prob) and assess the model accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm.default(formula = Crime ~ M + So + Po1 + Po2 + M.F + NW + \n",
       "    Ineq + Prob, data = uscrimes)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-503.51 -109.32   31.78  121.54  461.81 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -4793.3744  1292.3496  -3.709 0.000663 ***\n",
       "M              56.8152    39.7805   1.428 0.161398    \n",
       "So             76.9376   132.5291   0.581 0.564982    \n",
       "Po1           156.3860   102.7777   1.522 0.136389    \n",
       "Po2           -39.0842   112.4549  -0.348 0.730093    \n",
       "M.F            34.2062    12.8491   2.662 0.011319 *  \n",
       "NW             -0.6355     6.1847  -0.103 0.918697    \n",
       "Ineq           37.5800    17.6254   2.132 0.039518 *  \n",
       "Prob        -4584.9286  1926.7576  -2.380 0.022455 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 227.7 on 38 degrees of freedom\n",
       "Multiple R-squared:  0.7137,\tAdjusted R-squared:  0.6535 \n",
       "F-statistic: 11.84 on 8 and 38 DF,  p-value: 2.837e-08\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final.elastic.model <- lm(Crime ~ M + So+ Po1 + Po2 + M.F + NW + Ineq + Prob, data = uscrimes)\n",
    "summary(final.elastic.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most significant predictors are M + So + Po1 + Po2 + M.F + NW + Ineq + Prob. Adjusted R squared is 0.0.6535. p_value is much less than 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The performance of the different models - lasso and elastic net - can be compared using caret. The best model is defined as the one that minimizes the prediction error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "summary.resamples(object = ., metric = \"RMSE\")\n",
       "\n",
       "Models: lasso, elastic \n",
       "Number of resamples: 10 \n",
       "\n",
       "RMSE \n",
       "            Min.  1st Qu.  Median     Mean  3rd Qu.     Max. NA's\n",
       "lasso   88.60490 173.3667 269.260 263.0751 349.9370 413.1937    0\n",
       "elastic 78.29369 218.0573 259.314 267.5597 311.5064 495.1273    0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models <- list(lasso = lasso, elastic = elastic_model)\n",
    "resamples(models) %>% summary( metric = \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It is seen that Elastic Net model has the lowest median RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My sister-in-law is working as an Insights Analyst in Chicago. She is working on analyzing marketing insights from google trends data to study the target population that buys the company's products. I was suggesting that she can develop a scoring model that ranks customers/regions with highest to lowest google searches to product bought ratios. Creating an experiment like this will help her company categorize the regions that will be more responsive to marketing emails. This will help the company focus on regions that the product can be forecasted to sell the best. And a combination of google searches to product bought ratio along with data like demography can help accelarate the sales a lot better. It might also help the company understand better the purchasing power parity trends of a region so one region can be focussed for low cost products alone that the other regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppressWarnings(suppressMessages(install.packages(\"FrF2\", repos='http://cran.us.r-project.org', dependencies = TRUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressWarnings(suppressMessages(library(FrF2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Feature.1</th><th scope=col>Feature.2</th><th scope=col>Feature.3</th><th scope=col>Feature.4</th><th scope=col>Feature.5</th><th scope=col>Feature.6</th><th scope=col>Feature.7</th><th scope=col>Feature.8</th><th scope=col>Feature.9</th><th scope=col>Feature.10</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>1 </td><td>1 </td><td>1 </td></tr>\n",
       "\t<tr><td>1 </td><td>-1</td><td>1 </td><td>1 </td><td>-1</td><td>1 </td><td>-1</td><td>1 </td><td>-1</td><td>-1</td></tr>\n",
       "\t<tr><td>-1</td><td>1 </td><td>1 </td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>1 </td><td>-1</td></tr>\n",
       "\t<tr><td>-1</td><td>-1</td><td>-1</td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>-1</td><td>1 </td><td>-1</td></tr>\n",
       "\t<tr><td>-1</td><td>-1</td><td>1 </td><td>1 </td><td>1 </td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>1 </td></tr>\n",
       "\t<tr><td>-1</td><td>1 </td><td>1 </td><td>-1</td><td>-1</td><td>-1</td><td>1 </td><td>1 </td><td>-1</td><td>1 </td></tr>\n",
       "\t<tr><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>1 </td></tr>\n",
       "\t<tr><td>-1</td><td>1 </td><td>-1</td><td>1 </td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>-1</td><td>1 </td></tr>\n",
       "\t<tr><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>1 </td><td>1 </td><td>-1</td></tr>\n",
       "\t<tr><td>1 </td><td>1 </td><td>-1</td><td>1 </td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>-1</td></tr>\n",
       "\t<tr><td>1 </td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>-1</td><td>1 </td><td>1 </td></tr>\n",
       "\t<tr><td>1 </td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>-1</td></tr>\n",
       "\t<tr><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>1 </td><td>-1</td></tr>\n",
       "\t<tr><td>1 </td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>-1</td><td>-1</td><td>1 </td><td>1 </td></tr>\n",
       "\t<tr><td>1 </td><td>1 </td><td>1 </td><td>-1</td><td>1 </td><td>1 </td><td>1 </td><td>-1</td><td>-1</td><td>-1</td></tr>\n",
       "\t<tr><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>1 </td><td>1 </td><td>1 </td><td>1 </td><td>-1</td><td>1 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " Feature.1 & Feature.2 & Feature.3 & Feature.4 & Feature.5 & Feature.6 & Feature.7 & Feature.8 & Feature.9 & Feature.10\\\\\n",
       "\\hline\n",
       "\t 1  & -1 & -1 & 1  & -1 & -1 & 1  & 1  & 1  & 1 \\\\\n",
       "\t 1  & -1 & 1  & 1  & -1 & 1  & -1 & 1  & -1 & -1\\\\\n",
       "\t -1 & 1  & 1  & 1  & -1 & -1 & 1  & -1 & 1  & -1\\\\\n",
       "\t -1 & -1 & -1 & 1  & 1  & 1  & 1  & -1 & 1  & -1\\\\\n",
       "\t -1 & -1 & 1  & 1  & 1  & -1 & -1 & -1 & -1 & 1 \\\\\n",
       "\t -1 & 1  & 1  & -1 & -1 & -1 & 1  & 1  & -1 & 1 \\\\\n",
       "\t 1  & 1  & 1  & 1  & 1  & 1  & 1  & 1  & 1  & 1 \\\\\n",
       "\t -1 & 1  & -1 & 1  & -1 & 1  & -1 & -1 & -1 & 1 \\\\\n",
       "\t -1 & 1  & -1 & -1 & -1 & 1  & -1 & 1  & 1  & -1\\\\\n",
       "\t 1  & 1  & -1 & 1  & 1  & -1 & -1 & 1  & -1 & -1\\\\\n",
       "\t 1  & 1  & -1 & -1 & 1  & -1 & -1 & -1 & 1  & 1 \\\\\n",
       "\t 1  & -1 & -1 & -1 & -1 & -1 & 1  & -1 & -1 & -1\\\\\n",
       "\t -1 & -1 & 1  & -1 & 1  & -1 & -1 & 1  & 1  & -1\\\\\n",
       "\t 1  & -1 & 1  & -1 & -1 & 1  & -1 & -1 & 1  & 1 \\\\\n",
       "\t 1  & 1  & 1  & -1 & 1  & 1  & 1  & -1 & -1 & -1\\\\\n",
       "\t -1 & -1 & -1 & -1 & 1  & 1  & 1  & 1  & -1 & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Feature.1 | Feature.2 | Feature.3 | Feature.4 | Feature.5 | Feature.6 | Feature.7 | Feature.8 | Feature.9 | Feature.10 |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1  | -1 | -1 | 1  | -1 | -1 | 1  | 1  | 1  | 1  |\n",
       "| 1  | -1 | 1  | 1  | -1 | 1  | -1 | 1  | -1 | -1 |\n",
       "| -1 | 1  | 1  | 1  | -1 | -1 | 1  | -1 | 1  | -1 |\n",
       "| -1 | -1 | -1 | 1  | 1  | 1  | 1  | -1 | 1  | -1 |\n",
       "| -1 | -1 | 1  | 1  | 1  | -1 | -1 | -1 | -1 | 1  |\n",
       "| -1 | 1  | 1  | -1 | -1 | -1 | 1  | 1  | -1 | 1  |\n",
       "| 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  | 1  |\n",
       "| -1 | 1  | -1 | 1  | -1 | 1  | -1 | -1 | -1 | 1  |\n",
       "| -1 | 1  | -1 | -1 | -1 | 1  | -1 | 1  | 1  | -1 |\n",
       "| 1  | 1  | -1 | 1  | 1  | -1 | -1 | 1  | -1 | -1 |\n",
       "| 1  | 1  | -1 | -1 | 1  | -1 | -1 | -1 | 1  | 1  |\n",
       "| 1  | -1 | -1 | -1 | -1 | -1 | 1  | -1 | -1 | -1 |\n",
       "| -1 | -1 | 1  | -1 | 1  | -1 | -1 | 1  | 1  | -1 |\n",
       "| 1  | -1 | 1  | -1 | -1 | 1  | -1 | -1 | 1  | 1  |\n",
       "| 1  | 1  | 1  | -1 | 1  | 1  | 1  | -1 | -1 | -1 |\n",
       "| -1 | -1 | -1 | -1 | 1  | 1  | 1  | 1  | -1 | 1  |\n",
       "\n"
      ],
      "text/plain": [
       "   Feature.1 Feature.2 Feature.3 Feature.4 Feature.5 Feature.6 Feature.7\n",
       "1  1         -1        -1        1         -1        -1        1        \n",
       "2  1         -1        1         1         -1        1         -1       \n",
       "3  -1        1         1         1         -1        -1        1        \n",
       "4  -1        -1        -1        1         1         1         1        \n",
       "5  -1        -1        1         1         1         -1        -1       \n",
       "6  -1        1         1         -1        -1        -1        1        \n",
       "7  1         1         1         1         1         1         1        \n",
       "8  -1        1         -1        1         -1        1         -1       \n",
       "9  -1        1         -1        -1        -1        1         -1       \n",
       "10 1         1         -1        1         1         -1        -1       \n",
       "11 1         1         -1        -1        1         -1        -1       \n",
       "12 1         -1        -1        -1        -1        -1        1        \n",
       "13 -1        -1        1         -1        1         -1        -1       \n",
       "14 1         -1        1         -1        -1        1         -1       \n",
       "15 1         1         1         -1        1         1         1        \n",
       "16 -1        -1        -1        -1        1         1         1        \n",
       "   Feature.8 Feature.9 Feature.10\n",
       "1  1         1         1         \n",
       "2  1         -1        -1        \n",
       "3  -1        1         -1        \n",
       "4  -1        1         -1        \n",
       "5  -1        -1        1         \n",
       "6  1         -1        1         \n",
       "7  1         1         1         \n",
       "8  -1        -1        1         \n",
       "9  1         1         -1        \n",
       "10 1         -1        -1        \n",
       "11 -1        1         1         \n",
       "12 -1        -1        -1        \n",
       "13 1         1         -1        \n",
       "14 -1        1         1         \n",
       "15 -1        -1        -1        \n",
       "16 1         -1        1         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FrF2(nruns = 16, nfactors = 10, factor.names = c('Feature.1','Feature.2','Feature.3','Feature.4','Feature.5','Feature.6',\n",
    "                                                 'Feature.7','Feature.8','Feature.9','Feature.10'),\n",
    "     default.levels = c(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nruns = 16 (16 houses), nfactors = 10 (10 features).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above is the fractional factorial design for this experiment showing what set of features each house should have. -1: Don't Include and 1: Include features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of data I woud expect to follow the distributions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Binomial Distribution: Likelihood that Rafa Nadal will win over Roger Federer in French open in each meeting, with each meeting distributed once a year for the years both of them are active at the same time.\n",
    "\n",
    "2. Geometric Distribution: My grandparents kept having babies until they birthed a boy baby (finding gender of the unborn baby is illegal in India). The probability that they kept having girl child until my uncle was born.\n",
    "\n",
    "3. Poisson Distribution: Number of calls made to an IT Service business is an example of Poisson distribution.\n",
    "\n",
    "4. Exponential Distribution: Time between a particular IT help desk receiving calls is an example of exponential distribution.\n",
    "\n",
    "5. Weibull Distribution: At work, I have used Weibull distribution to predict when a Machine Component will fail (depending on the application). Has helped the company make replacements based on condition monitoring following a Weibull distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
